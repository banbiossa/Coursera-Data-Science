---
title: "Statistical Inference week4"
output: pdf_document
---

### Power  

the probability of rejecting the null hypothesis when false 1 - beta

beta is 2nd error.

```{r power, echo=FALSE, error=TRUE}
mu0 = 30
mua = 32
n = 16
sigma = 4
alpha = 0.05
z <- qnorm(1 - alpha)
pnorm(mu0 + z*sigma/sqrt(n), mean = mua, sd = sigma/sqrt(n), lower.tail = F)

mu0 = 30
mua = 32
sigma = 4
n = 16
z <- qnorm(1-alpha)
pnorm(mu0 + z*sigma/sqrt(n), mean = mu0, sd = sigma/sqrt(n), lower.tail = F)
pnorm(mu0 + z*sigma/sqrt(n), mean = mua, sd = sigma/sqrt(n), lower.tail = F)

library(manipulate)
mu0 = 30
myplot <- function(sigma, mua, n, alpha){
        g = ggplot(data.frame(mu = c(27,36)), aes(x = mu))
        g = g + stat_function(fun = dnorm, geom = "line",
                              args = list(mean = mu0, sd = sigma/sqrt(n)),
                              size = 2, col = "red")
        g = g + stat_function(fun = dnorm, geom = "line",
                              args = list(mean = mua, sd = sigma/sqrt(n)),
                              size = 2, col = "blue")
        xitc = mu0 + qnorm(1-alpha) * sigma /sqrt(n)
        g = g + geom_vline(xintercept = xitc, size =3)
        g
}
manipulate(
        myplot(sigma, mua, n, alpha),
        sigma = slider(1,10, step = 1, initial = 4),
        mua = slider(30,35, step = 1, initial = 32), 
        n = slider(1,50, step = 1, initial = 16),
        alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05)
        )
```

Usually you calculate for beta or n, to make a study, or make sure the study will be meaningful in a certain dataset.

#### t-test power

just use power.t.test (non-central t distruibution)
```{r t-test power}
power.t.test(n = 16, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$power
power.t.test(n = 16, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$power
power.t.test(n = 16, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$power

power.t.test(power = 0.8, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$n
power.t.test(power = 0.8, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$n
power.t.test(power = 0.8, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$n
```

you need n = 27 to detect a 0.5 differnece in the mean  

### Multiple comparisons

type1 : false positive  
type2 : false negative

\begin{center}
\begin{tabular}{ l c c r }
                        & $\beta = 0 $    & $\beta \neq 0  $& HYPOTHESES \\
        Claim $\beta = 0 $     & U        & T             & m-R\\
        Claim$ \beta \neq $0   & V        & S             & R\\
        Claims          &$ m_0$           & $m - m_0 $      & m\\
\end{tabular}
\end{center}

- false positive rate: $$ E \Biggl[ \frac{V}{m_0} \Biggr]$$
- familiy wise error rate: Pr(V>=1) the probabily of at least one false positive
- false discovery rate : $$ E \Biggl[ \frac{V}{R} \Biggr]$$

### controlling false positive  

#### Bonferroni correction  

set $$ \alpha_{fwer} = \alpha / m $$
where m is the number of tests. 
This is easy to calculate, but may be very conservative.  

### Controlling false discovery rate
- most popular in genomics, etc.
- do m tests
- order p values from smallest to largest $$P_{(1)}, \dots, P_{(m)}$$
- call any $$P_{(i)} \leq \alpha \times \frac{i}{m}$$ significant
- this will allow for more false positives

### Example with 10 p-values  

## Adjust the p values  
- not p values anymore!
- easier than adjusting alpha
- eg. p values P_1, ..., P_m
$$ P_i^{fwer} = max~m \times P_i,1 $$ for each P-value.
- Then if you call all $$P_i^{fwer} < \alpha $$ significant you will control the FWER.

```{r no true positives}
set.seed(1010093)
pValues <- rep(NA, 1000)
for(i in 1:1000){
        y <- rnorm(20)
        x <- rnorm(20)
        pValues[i] <- summary(lm(y~x))$coeff[2,4]
}
# control false positive rate
sum(pValues<0.05)
sum(p.adjust(pValues, method = "bonferroni")<0.05)
sum(p.adjust(pValues, method = "BH")<0.05)
```

```{r with relationship}
set.seed(1010093)
pValues <- rep(NA,1000)
for(i in 1:1000){
        x <- rnorm(20)
        #first 500, beta = 0, last 500 beta = 2
        if(i<=500){y <- rnorm(20)}else{y <- rnorm(20,mean=2*x)}
        pValues[i] <- summary(lm(y~x))$coeff[2,4]
}
trueStatus <- rep(c("zero","not zero"), each = 500)
table(pValues < 0.05, trueStatus)
table(p.adjust(pValues, method = "bonferroni") < 0.05, trueStatus)
table(p.adjust(pValues, method = "BH")<0.05, trueStatus)
```

```{r what p method does}
par(mfrow = c(1,2))
plot(pValues, p.adjust(pValues, method = "bonferroni"), pch = 19)
plot(pValues, p.adjust(pValues, method = "BH"), pch = 19)
```

maybe method = "BY" is good too

# Bootstrapping

Basic idea: when you have only one distruibution, use that over and over to sample randomly, to figure what kind of charestiristics it has. 

```{r dataset}
library(UsingR)
data(father.son)
x <- father.son$sheight
n <- length(x)
B <- 10000
resamples <- matrix(sample(x, n*B, replace = TRUE), B, n)
resampledMedians <- apply(resamples, 1, median)
hist(resampledMedians)
```

## How to do it
1. resample the whole data
2. take the mean
3. do it B times

```{r look further}
sd(resampledMedians)
quantile(resampledMedians, c(0.025, 0.975))

library(ggplot2)
g = ggplot(data.frame(medians = resampledMedians), aes(x = medians))
g = g + geom_histogram(color = "black", fill = "lightblue", binwidth = 0.05)
g
```

- better to take a bootstrap confidence intervals correct for bias
- "An introduction to bootstrap" is a good place to start

# Permutation tests

This is for comparing groups

- take a data frame with groups
- sample without the groups
- was the sample more extreme?

```{r permutation tests}
data(InsectSprays)
g = ggplot(InsectSprays, aes(spray, count, fill = spray))
g = g + geom_boxplot()
g
subdata <- InsectSprays[InsectSprays$spray %in% c("B","C"),]
y <- subdata$count
group <- as.character(subdata$spray)
teststat <- function(w,g) mean(w[g=="B"]) - mean(w[g=="C"])
observedStat <- teststat(y, group)
permutations <- sapply(1:10000, function(i) teststat(y, sample(group)))
observedStat
mean(permutations > observedStat)
hist(permutations)
g = ggplot(data.frame(permutations = permutations),
           aes(permutations))
g = g + geom_histogram(fill = "lightblue", color = "black", binwidth = 1)
g = g + geom_vline(xintercept = observedStat, size = 2)
g
```

# Quiz
Subject  Baseline	Week 2


a <- scan() 
1  140  132
b <- scan()
2	138	135
c <- scan()
3	150	151
d <- scan()
4	148	146
e <- scan()
5	135	130

data <- rbind(a,b,c,d,e)
data <- data.frame(data)
names(data) <- c("subject", "baseline", "week2")
data
t.test(data$baseline - data$week2)


```{r q22}
n = 9
mean = 1100
sd = 30
mean + c(-1,1)* qt(0.975,n-1) * sd/sqrt(n)
mean + c(-1,1)* qt(0.95,n-1) * sd
```

```{r q3}
a <- c(0,1,1,1)
t.test(a)
a <- a-1/2
t.test(a)
?binom.test
binom.test(3, 4, p = 0.5, alternative = "g")
```

```{r q4}
n = 10
d = 1787
b = 1/100
p = n/d
?pnorm
?ppois
ppois(p,b, lower.tail = F)
poisson.test(n, T = d, r = b, alternative = "l")
```

### q5  
- 18 patients, 9 on placebo, 9 on the drug  
- placebo: 1  sd: 1.8
- drug: -3  sd: 1.5
- do a two sided test

```{r two sided t}
?t.test
n <- 9
sdp <- 1.8
sdd <- 1.5
mp <- 1
md <- -3
sp <- sqrt( ((n - 1) * sdp ^2 + (n - 1) * sdd^2) / (n + n - 2))
mean <- md - mp
semd <- sp * sqrt(1/n + 1/n)
mean + c(-1,1) * qt(.975, n+n -2) * semd
pt(mean/semd, n+n-2)*2
mean/semd
mean
sp
semd
```

### q6
Researchers would like to conduct a study of 100 healthy adults to detect a four year mean brain volume loss of .01 mm3. Assume that the standard deviation of four year volume loss in this population is .04 mm3. About what would be the power of the study for a 5% one sided test versus a null hypothesis of no volume loss?

```{r test}
?power.t.test
power.t.test(n = 100, delta = 0.1,sd = 0.4, sig.level = 0.05, alternative = "one.sided",type = "one.sample")
```

### q8
Researchers would like to conduct a study of n healthy adults to detect a four year mean brain volume loss of .01 mm3. Assume that the standard deviation of four year volume loss in this population is .04 mm3. About what would be the value of n needded for 90% power of type one error rate of 5% one sided test versus a null hypothesis of no volume loss?

```{r needed for test}
power.t.test(delta = 0.1, sd = 0.4, sig.level = 0.05, power = 0.9, alternative = "one.sided", type = "one.sample")
